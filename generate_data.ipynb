{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329131ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92779cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format # Used to display float numbers in pandas with 0 decimal places and normal commas\n",
    "# Defining Time Range and Adjusting the dataframe for dataset\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "engagement_df = pd.DataFrame(index=date_range) # For creating a dataset\n",
    "\n",
    "daily_views = 350000  # Average Daily Views For a Big Content Creator Level Channel\n",
    "growth_rate = np.linspace(1,1.15,len(engagement_df)) # Simulate 15% growth every 2 years\n",
    "weekly_pattern =  np.array([0.95, 1.05, 1.05, 1.05, 1.05, 0.85, 0.9]) # Weekly boost (dip on Saturaday & Sunday)\n",
    "annual_pattern = np.sin(np.linspace(0, 2*np.pi*2, len(engagement_df))) * 0.10 + 1 # Generates a smooth yearly wave pattern(higher views during academic season and lower during breaks)- Around 10%\n",
    "random_fluctuations = np.random.normal(0, 25000, len(engagement_df)) # Adds random and unpredictable Fluctuations (0 - 25000 is done since a big channel is taken into account)\n",
    "engagement_df['Views'] = daily_views * growth_rate * weekly_pattern[engagement_df.index.dayofweek] * annual_pattern + random_fluctuations # Creates the inital Views Column by multiplying the 'daily_views' by the 'growth_rate', applies the 'weekly_pattern' and 'annual_pattern' based on the date, and then adds 'random_fluctuations'\n",
    "engagement_df['Views'] = engagement_df['Views'].apply(lambda x: max(50000, x)) # Ensuring no daily views count dont drop below 50000 ( Large Channel )\n",
    "\n",
    "release_frequency_days = 30 # Releasing a video every month\n",
    "release_dates = engagement_df.index[::release_frequency_days] # will help to identify specific dates in dataset to considered release dates\n",
    "\n",
    "# Using loop to apply spikes for new video release\n",
    "for date in release_dates:\n",
    "    # Simulate a large initial spike on release day for big channels\n",
    "    engagement_df.loc[date, 'Views'] += np.random.normal(2_000_000, 600_000) # Add 2M - 0.6M views\n",
    "\n",
    "    # Simulate decay over the next 30 days\n",
    "    for i in range(1, 31): # Loop from day 1 to day 30 after release\n",
    "        decay_factor = 0.95 ** i # Exponential decay, 5% less than previous day\n",
    "        if date + timedelta(days=i) <= engagement_df.index.max(): # Add decaying views for the days following the release\n",
    "            engagement_df.loc[date + timedelta(days=i), 'Views'] += np.random.normal(1_500_000 * decay_factor, 200_000) # Targets the view cell for the date we are calculating decay for\n",
    "\n",
    "# Convert 'Date' index back to a regular column for saving\n",
    "engagement_df = engagement_df.reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "# # Save your DataFrame to a CSV file\n",
    "# engagement_df.to_csv('simulated_physics_engagement.csv', index=False)\n",
    "\n",
    "# print(f\"Generated a dataset with {len(engagement_df)} rows and saved to 'simulated_physics_engagement.csv'.\")\n",
    "# print(\"First 5 rows of generated data:\")\n",
    "# print(engagement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f23b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_df = pd.read_csv(\"simulated_physics_engagement.csv\")\n",
    "engagement_df['Date'] = pd.to_datetime(engagement_df['Date']) # this line here convert the Date column from dataset into a datetime object by converting strings into a format which pandas can use to understand and perform datetime operations\n",
    "engagement_df = engagement_df.set_index('Date') # Make the Date column as index \n",
    "print('First five lines data after loading and indexing')\n",
    "print(engagement_df.head())\n",
    "print(f\"\\nThe DataFrame currently has {engagement_df.shape[0]} rows and {engagement_df.shape[1]} column(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8503732",
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_df['days_of_week'] = engagement_df.index.dayofweek #the method returns an integer with Monday=0,Tuesday=1 and since my data has weekly patterns\n",
    "engagement_df['days_of_year'] = engagement_df.index.dayofyear\n",
    "engagement_df['week_of_year'] = engagement_df.index.isocalendar().week.astype(int)\n",
    "engagement_df['month'] = engagement_df.index.month\n",
    "engagement_df['year'] = engagement_df.index.year\n",
    "\n",
    "engagement_df['is_weekend'] = engagement_df['days_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "engagement_df['quarter'] = engagement_df.index.quarter\n",
    "\n",
    "# Print a header for clarity.\n",
    "print(\"\\n--- Time-Based Features Added ---\")\n",
    "# Print the first 5 rows again to show the newly added columns.\n",
    "print(\"First 5 rows of your DataFrame with the new time features:\")\n",
    "print(engagement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9804d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#line creates 'views_lag_1', which contains the 'Views' from the previous day\n",
    "#shift()' is a Pandas method that moves the entire column down by n row.\n",
    "#typically the strongest predictor for current views because views tend to be correlated day-to-day.\n",
    "engagement_df['views_lag_1'] = engagement_df['Views'].shift(1) \n",
    "\n",
    "#This helps the model capture the weekly seasonality you've built into your data (e.g., Sunday views might be related to previous Sunday's views).\n",
    "engagement_df['views_lag_7'] = engagement_df['Views'].shift(7)\n",
    "\n",
    "for i in range(2, 7):\n",
    "    engagement_df[f'views_lag_{i}'] = engagement_df['Views'].shift(i)\n",
    "\n",
    "engagement_df['views_rolling_mean_7'] = engagement_df['Views'].rolling(window=7).mean().shift(1)\n",
    "\n",
    "engagement_df['views_rolling_std_7'] = engagement_df['Views'].rolling(window=7).std().shift(1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Lag and Rolling Mean Features Added ---\")\n",
    "\n",
    "print(\"First 10 rows with new lag and rolling features (notice 'NaN' values at the top):\")\n",
    "print(engagement_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684de06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rows_with_nan = engagement_df.isnull().sum().max()\n",
    "engagement_df.dropna(inplace=True)\n",
    "print(f\"\\n--- Handling Missing Values ---\")\n",
    "# Confirm how many rows were removed, which helps you understand the data loss.\n",
    "print(f\"Removed {initial_rows_with_nan} rows that contained NaN values (typically from lag features).\")\n",
    "# Print the final shape of the DataFrame after cleaning to show the reduced number of rows.\n",
    "print(f\"DataFrame shape after dropping NaNs: {engagement_df.shape}\")\n",
    "# Print the first 5 rows of the cleaned DataFrame to show that there are no more NaNs at the top.\n",
    "print(\"First 5 rows of the cleaned DataFrame (no NaNs):\")\n",
    "print(engagement_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d21727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I took January 1,2024 as the cut off date(split point). All data before this date will be for training,and all data from or after this date will be for testing.\n",
    "split_date = datetime(2024, 1, 1) # This means the model learns from all of 2023's data to predict 2024's data.\n",
    "clues  = engagement_df.drop('Views', axis = 1) #'engagement_df.drop('Views', axis=1)' means: take the whole table, and remove the 'Views' column.\n",
    "views_actual = engagement_df['Views'] #This is the column we want to predict â€“ 'Views\n",
    "\n",
    "#If a date is earlier than Jan 1, 2024, the result for that date will be 'True'.\n",
    "# If a date is on or after Jan 1, 2024, the result for that date will be 'False'.\n",
    "# This 'train_mask' will be a long list of True/False values, one for each date\n",
    "train_mask = clues.index < split_date  # Line created for T/F based on my dates , clues.index refers to dateindex of my dataframe. \n",
    "clues_train = clues[train_mask] # Only rows where train_mask is true will be selcted in clues_train i.e all dates in 2023\n",
    "views_actual_train = views_actual[train_mask] # To select corresponding target for training, will contain actual 'Views' for dates in 2023\n",
    "# This code lines will help me -(clues_train - the model learns from) and (views_actual_train - the correct answers my model learns from)\n",
    "\n",
    "test_mask = clues.index >= split_date # Indicator for which dates will be in my testing list, (Use it for testing- True and Dont - False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clues_test = clues[test_mask] # this will select the rows from clues table where the test_mask is true \n",
    "views_actual_test = views_actual[test_mask] #Will select the corresponding answers for test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bee5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Data Split Confirmed ---\")\n",
    "\n",
    "# Print the date range for your training data (X_train.index and y_train.index are the same).\n",
    "# '.min()' gets the earliest date, '.max()' gets the latest date.\n",
    "# '.strftime('%Y-%m-%d')' formats the date nicely as YYYY-MM-DD.\n",
    "print(f\"Training data range: {clues_train.index.min().strftime('%Y-%m-%d')} to {clues_train.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Do the same for your testing data.\n",
    "print(f\"Testing data range:  {clues_test.index.min().strftime('%Y-%m-%d')} to {clues_test.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Print the dimensions (shape) of your training sets.\n",
    "# '.shape' returns a tuple (number of rows, number of columns).\n",
    "print(f\"Training data size (rows, columns): {clues_train.shape}\")\n",
    "\n",
    "# Print the dimensions of your testing sets.\n",
    "print(f\"Testing data size (rows, columns): {clues_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e03b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Training (Using xgboost.train)\n",
    "\n",
    "import xgboost as xgb \n",
    "\n",
    "# This converts your Pandas DataFrames into XGBoost's highly optimized internal data format.\n",
    "dtrain = xgb.DMatrix(clues_train, label=views_actual_train)\n",
    "dvalid = xgb.DMatrix(clues_test, label=views_actual_test)\n",
    "\n",
    "# These are the configuration settings for your XGBoost model.\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\", # The model's goal: minimize squared prediction errors.\n",
    "    \"eval_metric\": \"rmse\",           # The metric to monitor during training for evaluation.\n",
    "    \"learning_rate\": 0.05,           # Controls how much the model adjusts its understanding each step.\n",
    "    \"seed\": 42,                      # Sets the random seed for reproducible results.\n",
    "}\n",
    "\n",
    "# These are the datasets XGBoost will check its performance on during training (train and validation/test).\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"validation\")]\n",
    "\n",
    "# This line starts the training process.\n",
    "model = xgb.train(\n",
    "    params,                          # The learning parameters.\n",
    "    dtrain,                          # Your training data in DMatrix format.\n",
    "    num_boost_round=1000,            # Maximum number of boosting rounds (trees).\n",
    "    evals=evals,                     # The evaluation sets for monitoring.\n",
    "    early_stopping_rounds=50,        # Stops training if performance doesn't improve for 50 rounds.\n",
    "    verbose_eval=True                # Set to True to see training progress in the output.\n",
    ")\n",
    "\n",
    "print(\"\\n--- XGBoost 'Brain' Trained Successfully ---\")\n",
    "# 'model.best_iteration' shows the optimal number of trees used.\n",
    "print(f\"Best iteration (number of trees used by the brain): {model.best_iteration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2529ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dvalid)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE).\n",
    "# It's the average absolute difference between the actual views (views_actual_test) and the predicted views (predictions).\n",
    "mae = mean_absolute_error(views_actual_test, predictions)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE).\n",
    "# First, we calculate Mean Squared Error (mean_squared_error), which squares the differences.\n",
    "# Then, we take the square root of that result (np.sqrt) to get RMSE, putting it back in the same units as views.\n",
    "rmse = np.sqrt(mean_squared_error(views_actual_test, predictions))\n",
    "\n",
    "# Print the calculated metrics, formatted nicely for readability.\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:,.0f}\") # Formatted with comma for thousands, 0 decimal places.\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:,.0f}\") # Formatted with comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc5f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize Actual vs. Predicted (Full Test Set) \n",
    "\n",
    "sns.set_style(\"darkgrid\") # Style for background plot\n",
    "\n",
    "plt.figure(figsize=(20, 10)) \n",
    "\n",
    "# Plot for ACTUAL views from your test set.\n",
    "# 'views_actual_test.index' provides the dates for the horizontal (x) axis.\n",
    "# 'views_actual_test' provides the actual view counts for the vertical (y) axis.\n",
    "plt.plot(views_actual_test.index, views_actual_test,\n",
    "label='Actual Views',     \n",
    "color='red',             \n",
    "alpha=0.7,                \n",
    "linewidth=2)             \n",
    "\n",
    "# Plot for PREDICTED views.\n",
    "# We use the same dates (views_actual_test.index) so predictions align with actuals.\n",
    "# 'predictions' is the array of guessed view counts from your model.\n",
    "plt.plot(views_actual_test.index, predictions,\n",
    "    label='Predicted Views',   \n",
    "    color='green',               \n",
    "    linestyle='-.',            \n",
    "    alpha=0.7,                 \n",
    "    linewidth=4)            \n",
    "\n",
    "# Add a title to your plot for clarity.\n",
    "plt.title('Actual vs. Predicted Daily Views (Full 2024 Test Set)', fontsize=18)\n",
    "# Add labels to your axes.\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.ylabel('Views', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2802a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Actual vs. Predicted (Zoomed In for first month)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot the ACTUAL views for only the first 30 days of the test set.\n",
    "# The index (dates) for these 30 days are accessed using 'views_actual_test.index[:30]'.\n",
    "plt.plot(views_actual_test.index[:30], views_actual_test.head(30),\n",
    "    label='Actual Views',\n",
    "    color='red',\n",
    "    alpha=0.7,                \n",
    "    linewidth=2.7)             \n",
    "\n",
    "plt.plot(views_actual_test.index[:30], predictions[:30],\n",
    "    label='Predicted Views',\n",
    "    color='green',\n",
    "    linestyle='-.',\n",
    "    alpha=0.7,\n",
    "    linewidth=2.7)\n",
    "\n",
    "\n",
    "plt.title('Actual vs. Predicted Daily Views (First 30 Days of Test Set)', fontsize=16)\n",
    "\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Views', fontsize=14)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36211cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp_scores_raw = model.get_score(importance_type='weight')\n",
    "imp_series = pd.Series(imp_scores_raw)\n",
    "imp_sorted = imp_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features for View Prediction\")\n",
    "print(imp_sorted.head(10))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=imp_sorted.head(10).values, \n",
    "    y = imp_sorted.head(10).index,\n",
    "    palette = 'plasma')\n",
    "\n",
    "plt.title('Top 10 Feature Importances for Daily Views', fontsize=16)\n",
    "plt.xlabel('Importance Score (Weight)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained model.\n",
    "# This command  'model.get_score()' method (for xgb.Booster objects) returns a dic\n",
    "# where keys are feature names and values are their importance scores.\n",
    "# It often uses 'weight' which is the number of times a feature is used in decision trees.\n",
    "feature_importances = model.get_score(importance_type='weight') # 'weight' is default, can also be 'gain', 'cover'\n",
    "\n",
    "# Convert the dictionary to a Pandas Series for easier sorting and plotting.\n",
    "feature_importances_series = pd.Series(feature_importances)\n",
    "\n",
    "# Sort the features by their importance in descending order (most important first).\n",
    "feature_importances_sorted = feature_importances_series.sort_values(ascending=False)\n",
    "\n",
    "# Print the top 10 most important features.\n",
    "print(\"--- Top 10 Most Important Features for Views Prediction ---\")\n",
    "print(feature_importances_sorted.head(10))\n",
    "\n",
    "# Visualize the top 10 most important features using a bar plot.\n",
    "# Make sure matplotlib.pyplot as plt and seaborn as sns are imported.\n",
    "# (These should be in your first cell already from your initial setup)\n",
    "\n",
    "plt.figure(figsize=(12, 7)) # Set the size of the plot.\n",
    "sns.barplot(x=feature_importances_sorted.head(10).values, # Values for the bars (importance scores).\n",
    "            y=feature_importances_sorted.head(10).index,   # Labels for the bars (feature names).\n",
    "            palette='viridis')                              # Color scheme for the bars.\n",
    "\n",
    "plt.title('Top 10 Feature Importances for Daily Views', fontsize=16) # Plot title.\n",
    "plt.xlabel('Importance Score (Weight)', fontsize=12)               # X-axis label.\n",
    "plt.ylabel('Feature', fontsize=12)                                 # Y-axis label.\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping.\n",
    "plt.show() # Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78cfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
