{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329131ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pandas numpy scikit-learn xgboost matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92779cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format # Used to display float numbers in pandas with 0 decimal places and normal commas\n",
    "# Defining Time Range and Adjusting the dataframe for dataset\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "engagement_df = pd.DataFrame(index=date_range) # For creating a dataset\n",
    "\n",
    "daily_views = 350000  # Average Daily Views For a Big Content Creator Level Channel\n",
    "growth_rate = np.linspace(1,1.15,len(engagement_df)) # Simulate 15% growth every 2 years\n",
    "weekly_pattern =  np.array([0.95, 1.05, 1.05, 1.05, 1.05, 0.85, 0.9]) # Weekly boost (dip on Saturaday & Sunday)\n",
    "annual_pattern = np.sin(np.linspace(0, 2*np.pi*2, len(engagement_df))) * 0.10 + 1 # Generates a smooth yearly wave pattern(higher views during academic season and lower during breaks)- Around 10%\n",
    "random_fluctuations = np.random.normal(0, 25000, len(engagement_df)) # Adds random and unpredictable Fluctuations (0 - 25000 is done since a big channel is taken into account)\n",
    "engagement_df['Views'] = daily_views * growth_rate * weekly_pattern[engagement_df.index.dayofweek] * annual_pattern + random_fluctuations # Creates the inital Views Column by multiplying the 'daily_views' by the 'growth_rate', applies the 'weekly_pattern' and 'annual_pattern' based on the date, and then adds 'random_fluctuations'\n",
    "engagement_df['Views'] = engagement_df['Views'].apply(lambda x: max(50000, x)) # Ensuring no daily views count dont drop below 50000 ( Large Channel )\n",
    "\n",
    "release_frequency_days = 30 # Releasing a video every month\n",
    "release_dates = engagement_df.index[::release_frequency_days] # will help to identify specific dates in dataset to considered release dates\n",
    "\n",
    "# Using loop to apply spikes for new video release\n",
    "for date in release_dates:\n",
    "    # Simulate a large initial spike on release day for big channels\n",
    "    engagement_df.loc[date, 'Views'] += np.random.normal(2_000_000, 600_000) # Add 2M - 0.6M views\n",
    "\n",
    "    # Simulate decay over the next 30 days\n",
    "    for i in range(1, 31): # Loop from day 1 to day 30 after release\n",
    "        decay_factor = 0.95 ** i # Exponential decay, 5% less than previous day\n",
    "        if date + timedelta(days=i) <= engagement_df.index.max(): # Add decaying views for the days following the release\n",
    "            engagement_df.loc[date + timedelta(days=i), 'Views'] += np.random.normal(1_500_000 * decay_factor, 200_000) # Targets the view cell for the date we are calculating decay for\n",
    "\n",
    "# Convert 'Date' index back to a regular column for saving\n",
    "engagement_df = engagement_df.reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "# # Save your DataFrame to a CSV file\n",
    "# engagement_df.to_csv('simulated_physics_engagement.csv', index=False)\n",
    "\n",
    "# print(f\"Generated a dataset with {len(engagement_df)} rows and saved to 'simulated_physics_engagement.csv'.\")\n",
    "# print(\"First 5 rows of generated data:\")\n",
    "# print(engagement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f23b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five lines data after loading and indexing\n",
      "               Views\n",
      "Date                \n",
      "2023-01-01 2,166,697\n",
      "2023-01-02 1,508,327\n",
      "2023-01-03 1,846,483\n",
      "2023-01-04 1,855,636\n",
      "2023-01-05 1,261,890\n",
      "\n",
      "The DataFrame currently has 731 rows and 1 column(s).\n"
     ]
    }
   ],
   "source": [
    "engagement_df = pd.read_csv(\"simulated_physics_engagement.csv\")\n",
    "engagement_df['Date'] = pd.to_datetime(engagement_df['Date']) # this line here convert the Date column from dataset into a datetime object by converting strings into a format which pandas can use to understand and perform datetime operations\n",
    "engagement_df = engagement_df.set_index('Date') # Make the Date column as index \n",
    "print('First five lines data after loading and indexing')\n",
    "print(engagement_df.head())\n",
    "print(f\"\\nThe DataFrame currently has {engagement_df.shape[0]} rows and {engagement_df.shape[1]} column(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8503732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Time-Based Features Added ---\n",
      "First 5 rows of your DataFrame with the new time features:\n",
      "               Views  days_of_week  days_of_year  week_of_year  month  year  \\\n",
      "Date                                                                          \n",
      "2023-01-01 2,166,697             6             1            52      1  2023   \n",
      "2023-01-02 1,508,327             0             2             1      1  2023   \n",
      "2023-01-03 1,846,483             1             3             1      1  2023   \n",
      "2023-01-04 1,855,636             2             4             1      1  2023   \n",
      "2023-01-05 1,261,890             3             5             1      1  2023   \n",
      "\n",
      "            is_weekend  quarter  \n",
      "Date                             \n",
      "2023-01-01           1        1  \n",
      "2023-01-02           0        1  \n",
      "2023-01-03           0        1  \n",
      "2023-01-04           0        1  \n",
      "2023-01-05           0        1  \n",
      "\n",
      "Now your DataFrame has 8 columns (more features!).\n"
     ]
    }
   ],
   "source": [
    "engagement_df['days_of_week'] = engagement_df.index.dayofweek #the method returns an integer with Monday=0,Tuesday=1 and since my data has weekly patterns\n",
    "engagement_df['days_of_year'] = engagement_df.index.dayofyear\n",
    "engagement_df['week_of_year'] = engagement_df.index.isocalendar().week.astype(int)\n",
    "engagement_df['month'] = engagement_df.index.month\n",
    "engagement_df['year'] = engagement_df.index.year\n",
    "\n",
    "engagement_df['is_weekend'] = engagement_df['days_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "engagement_df['quarter'] = engagement_df.index.quarter\n",
    "\n",
    "# Print a header for clarity.\n",
    "print(\"\\n--- Time-Based Features Added ---\")\n",
    "# Print the first 5 rows again to show the newly added columns.\n",
    "print(\"First 5 rows of your DataFrame with the new time features:\")\n",
    "print(engagement_df.head())\n",
    "# Print the updated DataFrame shape to confirm more columns have been added.\n",
    "print(f\"\\nNow your DataFrame has {engagement_df.shape[1]} columns (more features!).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9804d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lag and Rolling Mean Features Added ---\n",
      "First 10 rows with new lag and rolling features (notice 'NaN' values at the top):\n",
      "               Views  days_of_week  days_of_year  week_of_year  month  year  \\\n",
      "Date                                                                          \n",
      "2023-01-01 2,166,697             6             1            52      1  2023   \n",
      "2023-01-02 1,508,327             0             2             1      1  2023   \n",
      "2023-01-03 1,846,483             1             3             1      1  2023   \n",
      "2023-01-04 1,855,636             2             4             1      1  2023   \n",
      "2023-01-05 1,261,890             3             5             1      1  2023   \n",
      "2023-01-06 1,629,527             4             6             1      1  2023   \n",
      "2023-01-07 1,166,828             5             7             1      1  2023   \n",
      "2023-01-08 1,344,564             6             8             1      1  2023   \n",
      "2023-01-09 1,724,726             0             9             2      1  2023   \n",
      "2023-01-10 1,447,971             1            10             2      1  2023   \n",
      "\n",
      "            is_weekend  quarter  views_lag_1  views_lag_7  views_lag_2  \\\n",
      "Date                                                                     \n",
      "2023-01-01           1        1          NaN          NaN          NaN   \n",
      "2023-01-02           0        1    2,166,697          NaN          NaN   \n",
      "2023-01-03           0        1    1,508,327          NaN    2,166,697   \n",
      "2023-01-04           0        1    1,846,483          NaN    1,508,327   \n",
      "2023-01-05           0        1    1,855,636          NaN    1,846,483   \n",
      "2023-01-06           0        1    1,261,890          NaN    1,855,636   \n",
      "2023-01-07           1        1    1,629,527          NaN    1,261,890   \n",
      "2023-01-08           1        1    1,166,828    2,166,697    1,629,527   \n",
      "2023-01-09           0        1    1,344,564    1,508,327    1,166,828   \n",
      "2023-01-10           0        1    1,724,726    1,846,483    1,344,564   \n",
      "\n",
      "            views_lag_3  views_lag_4  views_lag_5  views_lag_6  \\\n",
      "Date                                                             \n",
      "2023-01-01          NaN          NaN          NaN          NaN   \n",
      "2023-01-02          NaN          NaN          NaN          NaN   \n",
      "2023-01-03          NaN          NaN          NaN          NaN   \n",
      "2023-01-04    2,166,697          NaN          NaN          NaN   \n",
      "2023-01-05    1,508,327    2,166,697          NaN          NaN   \n",
      "2023-01-06    1,846,483    1,508,327    2,166,697          NaN   \n",
      "2023-01-07    1,855,636    1,846,483    1,508,327    2,166,697   \n",
      "2023-01-08    1,261,890    1,855,636    1,846,483    1,508,327   \n",
      "2023-01-09    1,629,527    1,261,890    1,855,636    1,846,483   \n",
      "2023-01-10    1,166,828    1,629,527    1,261,890    1,855,636   \n",
      "\n",
      "            views_rolling_mean_7  views_rolling_std_7  \n",
      "Date                                                   \n",
      "2023-01-01                   NaN                  NaN  \n",
      "2023-01-02                   NaN                  NaN  \n",
      "2023-01-03                   NaN                  NaN  \n",
      "2023-01-04                   NaN                  NaN  \n",
      "2023-01-05                   NaN                  NaN  \n",
      "2023-01-06                   NaN                  NaN  \n",
      "2023-01-07                   NaN                  NaN  \n",
      "2023-01-08             1,633,627              353,687  \n",
      "2023-01-09             1,516,179              274,894  \n",
      "2023-01-10             1,547,093              285,815  \n"
     ]
    }
   ],
   "source": [
    "#line creates 'views_lag_1', which contains the 'Views' from the previous day\n",
    "#shift()' is a Pandas method that moves the entire column down by n row.\n",
    "#typically the strongest predictor for current views because views tend to be correlated day-to-day.\n",
    "engagement_df['views_lag_1'] = engagement_df['Views'].shift(1) \n",
    "\n",
    "#This helps the model capture the weekly seasonality you've built into your data (e.g., Sunday views might be related to previous Sunday's views).\n",
    "engagement_df['views_lag_7'] = engagement_df['Views'].shift(7)\n",
    "\n",
    "for i in range(2, 7):\n",
    "    engagement_df[f'views_lag_{i}'] = engagement_df['Views'].shift(i)\n",
    "\n",
    "engagement_df['views_rolling_mean_7'] = engagement_df['Views'].rolling(window=7).mean().shift(1)\n",
    "\n",
    "engagement_df['views_rolling_std_7'] = engagement_df['Views'].rolling(window=7).std().shift(1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Lag and Rolling Mean Features Added ---\")\n",
    "\n",
    "print(\"First 10 rows with new lag and rolling features (notice 'NaN' values at the top):\")\n",
    "print(engagement_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684de06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Missing Values ---\n",
      "Removed 7 rows that contained NaN values (typically from lag features).\n",
      "DataFrame shape after dropping NaNs: (724, 17)\n",
      "First 5 rows of the cleaned DataFrame (no NaNs):\n",
      "               Views  days_of_week  days_of_year  week_of_year  month  year  \\\n",
      "Date                                                                          \n",
      "2023-01-08 1,344,564             6             8             1      1  2023   \n",
      "2023-01-09 1,724,726             0             9             2      1  2023   \n",
      "2023-01-10 1,447,971             1            10             2      1  2023   \n",
      "2023-01-11 1,268,377             2            11             2      1  2023   \n",
      "2023-01-12 1,205,494             3            12             2      1  2023   \n",
      "\n",
      "            is_weekend  quarter  views_lag_1  views_lag_7  views_lag_2  \\\n",
      "Date                                                                     \n",
      "2023-01-08           1        1    1,166,828    2,166,697    1,629,527   \n",
      "2023-01-09           0        1    1,344,564    1,508,327    1,166,828   \n",
      "2023-01-10           0        1    1,724,726    1,846,483    1,344,564   \n",
      "2023-01-11           0        1    1,447,971    1,855,636    1,724,726   \n",
      "2023-01-12           0        1    1,268,377    1,261,890    1,447,971   \n",
      "\n",
      "            views_lag_3  views_lag_4  views_lag_5  views_lag_6  \\\n",
      "Date                                                             \n",
      "2023-01-08    1,261,890    1,855,636    1,846,483    1,508,327   \n",
      "2023-01-09    1,629,527    1,261,890    1,855,636    1,846,483   \n",
      "2023-01-10    1,166,828    1,629,527    1,261,890    1,855,636   \n",
      "2023-01-11    1,344,564    1,166,828    1,629,527    1,261,890   \n",
      "2023-01-12    1,724,726    1,344,564    1,166,828    1,629,527   \n",
      "\n",
      "            views_rolling_mean_7  views_rolling_std_7  \n",
      "Date                                                   \n",
      "2023-01-08             1,633,627              353,687  \n",
      "2023-01-09             1,516,179              274,894  \n",
      "2023-01-10             1,547,093              285,815  \n",
      "2023-01-11             1,490,163              254,180  \n",
      "2023-01-12             1,406,269              205,749  \n"
     ]
    }
   ],
   "source": [
    "initial_rows_with_nan = engagement_df.isnull().sum().max()\n",
    "engagement_df.dropna(inplace=True)\n",
    "print(f\"\\n--- Handling Missing Values ---\")\n",
    "# Confirm how many rows were removed, which helps you understand the data loss.\n",
    "print(f\"Removed {initial_rows_with_nan} rows that contained NaN values (typically from lag features).\")\n",
    "# Print the final shape of the DataFrame after cleaning to show the reduced number of rows.\n",
    "print(f\"DataFrame shape after dropping NaNs: {engagement_df.shape}\")\n",
    "# Print the first 5 rows of the cleaned DataFrame to show that there are no more NaNs at the top.\n",
    "print(\"First 5 rows of the cleaned DataFrame (no NaNs):\")\n",
    "print(engagement_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d21727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I took January 1,2024 as the cut off date(split point). All data before this date will be for training,and all data from or after this date will be for testing.\n",
    "split_date = datetime(2024, 1, 1) # This means the model learns from all of 2023's data to predict 2024's data.\n",
    "clues  = engagement_df.drop('Views', axis = 1) #'engagement_df.drop('Views', axis=1)' means: take the whole table, and remove the 'Views' column.\n",
    "views_actual = engagement_df['Views'] #This is the column we want to predict â€“ 'Views\n",
    "\n",
    "#If a date is earlier than Jan 1, 2024, the result for that date will be 'True'.\n",
    "# If a date is on or after Jan 1, 2024, the result for that date will be 'False'.\n",
    "# This 'train_mask' will be a long list of True/False values, one for each date\n",
    "train_mask = clues.index < split_date  # Line created for T/F based on my dates , clues.index refers to dateindex of my dataframe. \n",
    "clues_train = clues[train_mask] # Only rows where train_mask is true will be selcted in clues_train i.e all dates in 2023\n",
    "views_actual_train = views_actual[train_mask] # To select corresponding target for training, will contain actual 'Views' for dates in 2023\n",
    "# This code lines will help me -(clues_train - the model learns from) and (views_actual_train - the correct answers my model learns from)\n",
    "\n",
    "test_mask = clues.index >= split_date # Indicator for which dates will be in my testing list, (Use it for testing- True and Dont - False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3e94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clues_test = clues[test_mask] # this will select the rows from clues table where the test_mask is true \n",
    "views_actual_test = views_actual[test_mask] #Will select the corresponding answers for test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2bee5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Split Confirmed ---\n",
      "Training data range: 2023-01-08 to 2023-12-31\n",
      "Testing data range:  2024-01-01 to 2024-12-31\n",
      "Training data size (rows, columns): (358, 16)\n",
      "Testing data size (rows, columns): (366, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Split Confirmed ---\")\n",
    "\n",
    "# Print the date range for your training data (X_train.index and y_train.index are the same).\n",
    "# '.min()' gets the earliest date, '.max()' gets the latest date.\n",
    "# '.strftime('%Y-%m-%d')' formats the date nicely as YYYY-MM-DD.\n",
    "print(f\"Training data range: {clues_train.index.min().strftime('%Y-%m-%d')} to {clues_train.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Do the same for your testing data.\n",
    "print(f\"Testing data range:  {clues_test.index.min().strftime('%Y-%m-%d')} to {clues_test.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Print the dimensions (shape) of your training sets.\n",
    "# '.shape' returns a tuple (number of rows, number of columns).\n",
    "print(f\"Training data size (rows, columns): {clues_train.shape}\")\n",
    "\n",
    "# Print the dimensions of your testing sets.\n",
    "print(f\"Testing data size (rows, columns): {clues_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e03b87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clues_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m \u001b[38;5;66;03m# Make sure xgboost is imported for this part.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Part 1: Convert your data to DMatrix format\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This converts your Pandas DataFrames into XGBoost's highly optimized internal data format.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(clues_train, label\u001b[38;5;241m=\u001b[39mviews_actual_train)\n\u001b[1;32m      8\u001b[0m dvalid \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(clues_test, label\u001b[38;5;241m=\u001b[39mviews_actual_test)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Part 2: Define Parameters for Training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# These are the configuration settings for your XGBoost model.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clues_train' is not defined"
     ]
    }
   ],
   "source": [
    "# CODE BLOCK 6: Model Selection and Training (Using xgboost.train)\n",
    "\n",
    "import xgboost as xgb # Make sure xgboost is imported for this part.\n",
    "\n",
    "# Part 1: Convert your data to DMatrix format\n",
    "# This converts your Pandas DataFrames into XGBoost's highly optimized internal data format.\n",
    "dtrain = xgb.DMatrix(clues_train, label=views_actual_train)\n",
    "dvalid = xgb.DMatrix(clues_test, label=views_actual_test)\n",
    "\n",
    "# Part 2: Define Parameters for Training\n",
    "# These are the configuration settings for your XGBoost model.\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\", # The model's goal: minimize squared prediction errors.\n",
    "    \"eval_metric\": \"rmse\",           # The metric to monitor during training for evaluation.\n",
    "    \"learning_rate\": 0.05,           # Controls how much the model adjusts its understanding each step.\n",
    "    \"seed\": 42,                      # Sets the random seed for reproducible results.\n",
    "}\n",
    "\n",
    "# Part 3: Set up Evaluation Sets for Monitoring\n",
    "# These are the datasets XGBoost will check its performance on during training (train and validation/test).\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"validation\")]\n",
    "\n",
    "# Part 4: Train the Model with Early Stopping\n",
    "# This line starts the training process.\n",
    "model = xgb.train(\n",
    "    params,                          # The learning parameters.\n",
    "    dtrain,                          # Your training data in DMatrix format.\n",
    "    num_boost_round=1000,            # Maximum number of boosting rounds (trees).\n",
    "    evals=evals,                     # The evaluation sets for monitoring.\n",
    "    early_stopping_rounds=50,        # Stops training if performance doesn't improve for 50 rounds.\n",
    "    verbose_eval=True                # Set to True to see training progress in the output.\n",
    ")\n",
    "\n",
    "print(\"\\n--- XGBoost 'Brain' Trained Successfully ---\")\n",
    "# 'model.best_iteration' shows the optimal number of trees used.\n",
    "print(f\"Best iteration (number of trees used by the brain): {model.best_iteration}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
